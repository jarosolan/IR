# Libraries import
import requests
import numpy as np
import time #sleep() would help to control the loop's rate
from time import sleep
from bs4 import BeautifulSoup
from elasticsearch import Elasticsearch
#import urllib2
"""CHECK of ROBOTS.TXT
https://pureportal.coventry.ac.uk/robots.txt
User-Agent: *
Crawl-Delay: 1
Disallow: /*?*format=rss
Disallow: /*?*export=xls
Sitemap: https://pureportal.coventry.ac.uk/s"""
# CRAWLER
start = time.time() # for further summary time of processing
url = 'https://pureportal.coventry.ac.uk/en/organisations/faculty-of-engineering-environment-computing/publications/?page='
articles1 = [] 
visited_list = [] #storing results, below should be 97 [I checked it manually]
for page in np.arange(0, 98, 1): 
    print()
    print('Visited page:', url + str(page))
    visited_list.append(url + str(page))
    page = requests.get(url + str(page))
    soup = BeautifulSoup(page.text, 'html.parser')
    sleep(1) # to control speed of scrapping, should be 1s as per robots.txt
    # loop for crawling
    for item in soup.find_all(class_='result-container'):
        item_a = item.find(class_ = 'link person') #+
        item_b = item.find(class_ = 'date') #+
        item_c = item.find(class_ = 'title') #+
        item_d = item.find(class_ = 'link') #+
        articles1.append({'Author': item_a,
                          'Date': item_b.text,
                          'Title': item_c.text,
                          'Link': item_d.get('href')})
    for article in articles1:
        print()
        print('Author:', article['Author'])
        print('Date:', article['Date'])
        print('Title:', article['Title'])
        print('Link:', article['Link'])
end = time.time()
print()
print('Total time of crawling execution:', end - start,'seconds')
print()
print(visited_list) 
# To check if Author is not null to be classified as a CU Researcher[E in VIVA]
print()
D = [d for d in articles1 if d['Author']]
print(D)
# Adjusting author column into 2 columns, author & link
articles2 = []
for index in D:
    item_1 = index['Author'].find('span').text
    item_2 = index['Date']
    item_3 = index['Title']
    item_4 = index['Link']
    item_5 = index['Author'].get('href')
    articles2.append({'Author': item_1,
                      'Date': item_2,
                      'Title': item_3,
                      'PublicaionLink': item_4,
                      'PurePortalProfile': item_5})
    for index in articles2:
        print('-------------')
        print('Author:', index['Author'])
        print('Date:', index['Date'])
        print('Title:', index['Title'])
        print('PublicaionLink:', index['PublicaionLink'])
        print('PurePortalProfile:', index['PurePortalProfile'])      

# INDEXER
es = Elasticsearch([{'host':'localhost','port':9200}])

i = 0
while(i in range(0, len(articles2))):
    z = articles2[i]
    res = es.index(index= 'titl',
               doc_type = 'papers', 
               id=i,
               body=z)
    i = i+1
print()
print(res['result'])

# QUERY PROCESSOR
start_time = time.time()
print('Please type searching query:')
search = input()  # input search
print()
print('Please type excluding query:')
search_exc = input()  # input what to exclude
res= es.search(index='titl',
               body= {
                   'query': {
                       'bool': {
                           'must': [ 
                               {'match': {'Title': search} }
                               ],
                               'must_not': {
                                   'bool': {
                                        'must': [ 
                                          {'match': {'Title': search_exc} }
                                            ]
                                        }
                                    }
                                }
                            }
                        }
               )                       

print('Search results for:', search, '\n','Excluding:', search_exc)

print('Input match the following results: ', res['hits']['total']['value'],
      'No. publications')
# Results
listing = 1
for hit in res['hits']['hits']:
    print(listing)
    print('Title:', hit['_source']['Title'])
    print('PublicaionLink:',  hit['_source']['PublicaionLink'])
    print('Author:', hit['_source']['Author'])
    print('PurePortalProfile:', hit['_source']['PurePortalProfile'])
    print('Date:', hit['_source']['Date'])
    print(hit['_score'])
    print('-------------')
    listing += 1  
    
end_time = time.time()
print('Total time of quering execution: {}'.format(end_time - start_time),
      'seconds')